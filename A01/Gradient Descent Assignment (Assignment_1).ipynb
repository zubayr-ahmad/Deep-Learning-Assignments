{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1225ed22e48e29fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradient Descent Assignment\n",
    "In this assignment, you will implement and experiment with Gradient Descent on a simulated dataset. Your task is to complete the missing sections of the code, understand the role of each part of the algorithm, and tweak the hyperparameters to see their effect.\n",
    "\n",
    "## Objectives:\n",
    "Understand and implement the Gradient Descent algorithm.\n",
    "Visualize how the cost function decreases with iterations.\n",
    "Tweak learning rates and number of iterations to observe the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbbf2854c784d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Instructions:\n",
    "- Complete the code where indicated.\n",
    "- Run each cell to observe the results.\n",
    "- Use assert statements to validate your code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0077823c522520",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 1: Setting up the Dataset\n",
    "In this section, we'll import necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f37126c6e19b83",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated real-world dataset with multiple features:\n",
    "X = np.array([\n",
    "    [750, 3, 30, 15],\n",
    "    [850, 3, 25, 12],\n",
    "    [900, 4, 20, 10],\n",
    "    [1200, 4, 18, 8],\n",
    "    [1500, 5, 10, 5],\n",
    "    [2000, 5, 5, 3],\n",
    "    [2300, 6, 3, 2],\n",
    "    [3000, 6, 2, 1],\n",
    "    [3500, 7, 1, 1],\n",
    "    [4000, 8, 1, 0]\n",
    "])  # Features: [Size, Bedrooms, Age, Distance to City Center]\n",
    "\n",
    "y = np.array([150, 175, 200, 300, 375, 480, 525, 650, 720, 800])  # Price of the house in 1000s of dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958181ae1c5d76a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this section, weâ€™ll import necessary libraries and load the dataset.\n",
    "\n",
    "$$ X_{norm} = \\frac{X - \\mu}{\\sigma} $$\n",
    " \n",
    "where mu is the mean and sigma is the standard deviation of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162bdbb14362031f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize features for better convergence\n",
    "X_norm = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "# Add a column of ones to X_norm to account for theta0 (intercept)\n",
    "X_b = np.c_[np.ones(X_norm.shape[0]), X_norm]  # Add intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ae14d24bf7cd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 2: Gradient Descent Algorithm\n",
    "Now, we will implement the Gradient Descent algorithm.\n",
    "\n",
    "### Useful Equations\n",
    "\n",
    "$$ Y_{prediction} = f(X) = W.X $$\n",
    "\n",
    "$$ J (Cost Function) = \\frac{1}{2m}  (Y_{prediction}  - Y)^2 $$\n",
    "\n",
    "$$ \\begin{align}\n",
    "Gradient &(\\frac{\\partial J(W)}{\\partial W}) = \\frac{1}{m}  (Y_{prediction} - Y).X\\\\\n",
    "\\end{align} $$\n",
    "\n",
    "$$ \\begin{align*} \n",
    "  W &= W -  \\alpha .(\\frac{\\partial J(W)}{\\partial W}) \n",
    "  \\end{align*} $$\n",
    "\n",
    "Complete the missing parts of the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b796029a7f2e26",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize parameters (weights vector) with zeros\n",
    "theta = np.zeros(X_b.shape[1])\n",
    "\n",
    "# Define learning rate and number of iterations\n",
    "alpha = 0.01  # You can experiment with this\n",
    "iterations = 1000  # Try changing this value\n",
    "\n",
    "# Gradient descent function\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)  # Number of training examples\n",
    "    cost_history = []  # To store the cost at each iteration\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Predicted values \n",
    "        y_pred = np.dot(X, theta)\n",
    "\n",
    "        # Compute the error (difference between predicted and actual values)\n",
    "        error = ___________\n",
    "\n",
    "        # Compute gradients \n",
    "        gradients = ___________\n",
    "\n",
    "        # Update the parameters (theta)\n",
    "        theta = ___________\n",
    "\n",
    "        # Compute the cost function (Mean Squared Error) and append to history\n",
    "        cost = ___________\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941097bd79b17f4d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Running the gradient descent function\n",
    "theta, cost_history = gradient_descent(X_b, y, theta, alpha, iterations)\n",
    "\n",
    "# Assert statement to check theta shape\n",
    "assert theta.shape == (X_b.shape[1],), \"Theta shape mismatch!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fe186a5c0d3b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 3: Visualizing the Cost Function\n",
    "Let's plot the cost function to observe how it decreases over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed824ce9655c6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(iterations), cost_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Over Iterations')\n",
    "plt.show()\n",
    "\n",
    "# Assert to check the final cost decreases\n",
    "assert cost_history[-1] < cost_history[0], \"Cost did not decrease!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996143ae386b99e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 4: Predictions and Visualization\n",
    "Now, we'll use the final parameters to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977878f40b0b9939",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predicted prices using the final parameters\n",
    "y_pred_final = X_b.dot(theta)\n",
    "\n",
    "# Plotting actual vs predicted prices\n",
    "plt.scatter(range(len(y)), y, color='blue', label='Actual Prices')\n",
    "plt.plot(range(len(y)), y_pred_final, color='red', label='Predicted Prices')\n",
    "plt.xlabel('House Index')\n",
    "plt.ylabel('Price (in 1000s of dollars)')\n",
    "plt.title('Actual vs Predicted House Prices')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Assert to check predictions length\n",
    "assert len(y_pred_final) == len(y), \"Prediction length mismatch!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482182c3dc4c96c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 6: Implementing Gradient Descent on Real-World Data (Ames Housing Dataset)\n",
    "In this section, you will apply the Gradient Descent algorithm to a real dataset from the Ames Housing dataset. Download the dataset from Kaggle and follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79235bb79e06d6b6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Instructions:\n",
    "Load the dataset and manually preprocess the data without using any third-party libraries like sklearn.\n",
    "Implement the Gradient Descent algorithm as you did for the simulated data.\n",
    "Visualize the cost function and make predictions.\n",
    "Experiment with hyperparameters and observe their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d04e88c3b499e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading and Preprocessing the Dataset (Manual Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c0ade3544880d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming the dataset is downloaded and named 'train.csv'\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Select a few features (you can try different sets of features)\n",
    "\n",
    "## Select Features by your own choice\n",
    "features = []\n",
    "target = 'SalePrice'\n",
    "\n",
    "# Extracting features and target\n",
    "X_real = df[features].values\n",
    "y_real = df[target].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_real, y_real, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Add a column of ones for the intercept term\n",
    "X_train_b = ___________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f38756321a5ffd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implementing Gradient Descent for Real-World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1c367777c5353",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize parameters (weights vector) with zeros\n",
    "theta_real = np.zeros(X_train_b.shape[1])\n",
    "\n",
    "# Define learning rate and number of iterations (You can experiment with these values)\n",
    "alpha_real = 0.01\n",
    "iterations_real = 1000\n",
    "\n",
    "## Incase of having NaN values in the dataset, replace them with 0\n",
    "X_train_b = np.nan_to_num(X_train_b)  # Replace NaNs with 0\n",
    "\n",
    "# Use the gradient_descent function you implemented earlier\n",
    "theta_real, cost_history_real = gradient_descent( ___________ )\n",
    "\n",
    "# Assert statement to check theta shape\n",
    "assert theta_real.shape == (X_train_b.shape[1],), \"Theta shape mismatch for real dataset!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b0193c7516f6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Visualizing the Cost Function for Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab758de0eae628",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the cost function over iterations for real data\n",
    "plt.plot(range(iterations_real), cost_history_real)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Over Iterations (Real Data)')\n",
    "plt.show()\n",
    "\n",
    "# Assert to check cost reduction\n",
    "assert cost_history_real[-1] < cost_history_real[0], \"Cost did not decrease for real data!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982641fbc2cf0a5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making Predictions on Real Data\n",
    "Once the model has been trained, we can now make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9b1cc4914267a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add intercept term for the test data\n",
    "X_test_b = ___________\n",
    "\n",
    "# Predicted prices using the final parameters for real data\n",
    "y_pred_real = ___________\n",
    "\n",
    "\n",
    "# Plotting actual vs predicted prices\n",
    "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual Prices')\n",
    "plt.plot(range(len(y_test)), y_pred_real, color='red', label='Predicted Prices')\n",
    "plt.xlabel('House Index')\n",
    "plt.ylabel('Price (in dollars)')\n",
    "plt.title('Actual vs Predicted House Prices (Real Data)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Assert to check predictions length\n",
    "assert len(y_pred_real) == len(y_test), \"Prediction length mismatch for real data!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fc006b1959e54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 7: Experimenting with Hyperparameters for Real Data\n",
    "You can now experiment with different learning rates and iterations to observe their effects on the model.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "Run the Gradient Descent with alpha=0.001, alpha=0.1, and alpha=0.05. Record the final cost and observe the impact of the learning rate.\n",
    "Try using different sets of features from the dataset. For example, you can add more features or remove certain features and see how the model's performance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce826c4344a94e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Final Note on Gradient Descent\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning by iteratively adjusting model parameters. The algorithm calculates the gradient (slope) of the cost function and updates the parameters in the direction that reduces the error.\n",
    "\n",
    "- Learning Rate (Î±) controls the size of steps taken towards the minimum.\n",
    "- Iterations determine how many times the parameters are updated.\n",
    "- Convergence is reached when changes in the cost become negligible.\n",
    "\n",
    "Gradient Descent has variants like Batch, Stochastic, and Mini-Batch, each balancing between computation time and convergence stability. It is widely used in linear regression, neural networks, and more, making it essential for optimizing machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6c50710d58754",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
